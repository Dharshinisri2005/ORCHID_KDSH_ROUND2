{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250114_060234\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.2\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       3.22 GB / 15.69 GB (20.5%)\n",
      "Disk Space Avail:   451.92 GB / 815.10 GB (55.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"c:\\Users\\harsh\\OneDrive\\Desktop\\hack iitk\\AutogluonModels\\ag-20250114_060234\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 2\n",
      "Label Column:       Publishable\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Warning: Updated label_count_threshold from 10 to 8 to avoid cutting too many classes.\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3627.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.77 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Rationale']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 1358\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', [])       : 1 | ['Conference']\n",
      "\t\t('object', ['text']) : 1 | ['Rationale']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :    1 | ['Conference']\n",
      "\t\t('category', ['text_as_category'])  :    1 | ['Rationale']\n",
      "\t\t('int', ['binned', 'text_special']) :   28 | ['Rationale.char_count', 'Rationale.word_count', 'Rationale.capital_ratio', 'Rationale.lower_ratio', 'Rationale.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 1349 | ['__nlp__.10', '__nlp__.100', '__nlp__.11', '__nlp__.12', '__nlp__.13', ...]\n",
      "\t9.4s = Fit runtime\n",
      "\t2 features in original data used to generate 1379 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.31 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 9.7s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 96, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t0.96s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t1.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t18.83s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t1.26s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t1.72s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\tColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9167\t = Validation score   (accuracy)\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 40.42s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 480.1 rows/s (24 batch size)\n",
      "Disabling decision threshold calibration for metric `accuracy` due to having fewer than 10000 rows of validation data for calibration, to avoid overfitting (24 rows).\n",
      "\t`accuracy` is generally not improved through threshold calibration. Force calibration via specifying `calibrate_decision_threshold=True`.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\harsh\\OneDrive\\Desktop\\hack iitk\\AutogluonModels\\ag-20250114_060234\")\n",
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250114_060315\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.2\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.05 GB / 15.69 GB (25.8%)\n",
      "Disk Space Avail:   451.88 GB / 815.10 GB (55.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='experimental' : New in v1.2: Pre-trained foundation model + parallel fits. The absolute best accuracy without consideration for inference speed. Does not support GPU.\n",
      "\tpresets='best'         : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'         : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'         : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'       : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"c:\\Users\\harsh\\OneDrive\\Desktop\\hack iitk\\AutogluonModels\\ag-20250114_060315\"\n",
      "Train Data Rows:    120\n",
      "Train Data Columns: 2\n",
      "Label Column:       Conference\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\t6 unique label values:  ['Non-Publishable', 'KDD', 'TMLR', 'CVPR', 'EMNLP', 'NeurIPS']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Warning: Updated label_count_threshold from 10 to 2 to avoid cutting too many classes.\n",
      "Warning: Some classes in the training set have fewer than 2 examples. AutoGluon will only keep 4 out of 6 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 2 examples that will be kept for training models: 0.9833333333333333\n",
      "Train Data Class Count: 4\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4139.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.51 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Rationale']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 1319\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])          : 1 | ['Publishable']\n",
      "\t\t('object', ['text']) : 1 | ['Rationale']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', ['text_as_category'])  :    1 | ['Rationale']\n",
      "\t\t('int', ['binned', 'text_special']) :   28 | ['Rationale.char_count', 'Rationale.word_count', 'Rationale.capital_ratio', 'Rationale.lower_ratio', 'Rationale.digit_ratio', ...]\n",
      "\t\t('int', ['bool'])                   :    1 | ['Publishable']\n",
      "\t\t('int', ['text_ngram'])             : 1310 | ['__nlp__.10', '__nlp__.100', '__nlp__.11', '__nlp__.12', '__nlp__.13', ...]\n",
      "\t10.1s = Fit runtime\n",
      "\t2 features in original data used to generate 1340 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.30 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 10.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 94, Val Rows: 24\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 7: early stopping\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t1.63s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tMany features detected (1340), dynamically setting 'colsample_bylevel' to 0.746268656716418 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t14.59s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training... Skipping this model.\n",
      "\t\tColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 201, in _fit\n",
      "    train_dataset = self._generate_dataset(X, y, train_params=processor_kwargs, is_train=True)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 669, in _generate_dataset\n",
      "    dataset = self._process_train_data(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 734, in _process_train_data\n",
      "    self.processor = create_preprocessor(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\utils\\data_preprocessor.py\", line 40, in create_preprocessor\n",
      "    return ColumnTransformer(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ColumnTransformer.__init__() got an unexpected keyword argument 'force_int_remainder_cols'\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.9583\t = Validation score   (accuracy)\n",
      "\t4.07s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (accuracy)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 41.04s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 838.0 rows/s (24 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\harsh\\OneDrive\\Desktop\\hack iitk\\AutogluonModels\\ag-20250114_060315\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishability Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.68      0.79        28\n",
      "           1       0.10      0.50      0.17         2\n",
      "\n",
      "    accuracy                           0.67        30\n",
      "   macro avg       0.53      0.59      0.48        30\n",
      "weighted avg       0.89      0.67      0.75        30\n",
      "\n",
      "Conference Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "           CVPR       0.00      0.00      0.00         1\n",
      "          EMNLP       0.00      0.00      0.00         1\n",
      "Non-Publishable       0.93      1.00      0.97        28\n",
      "\n",
      "       accuracy                           0.93        30\n",
      "      macro avg       0.31      0.33      0.32        30\n",
      "   weighted avg       0.87      0.93      0.90        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\harsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Result for Example Document: Publishable, Conference: Non-Publishable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to extract text from PDFs\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    data = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                reader = PdfReader(file_path)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text()\n",
    "                # Extract category and conference from folder structure\n",
    "                parts = root.split(os.sep)\n",
    "                if len(parts) >= 2:\n",
    "                    publishable_status = parts[-2]\n",
    "                    conference = parts[-1] if publishable_status == \"Publishable\" else \"Non-Publishable\"\n",
    "                else:\n",
    "                    publishable_status = \"Unknown\"\n",
    "                    conference = \"Unknown\"\n",
    "                paper_id = os.path.splitext(file)[0]\n",
    "                is_publishable = 1 if publishable_status == \"Publishable\" else 0\n",
    "                data.append({'Paper ID': paper_id, 'Publishable': is_publishable, 'Conference': conference, 'Rationale': text})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Function to extract text from DOCX files\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text = \"\"\n",
    "    for para in doc.paragraphs:\n",
    "        text += para.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Path to the folder containing PDFs and DOCX files\n",
    "pdf_folder_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\hack iitk\\KDSH_2025_Dataset-20250106T145006Z-001\\KDSH_2025_Dataset\"\n",
    "\n",
    "# Extract text and create dataset\n",
    "data = extract_text_from_pdfs(pdf_folder_path)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = data[['Rationale']]\n",
    "y = data[['Publishable', 'Conference']]\n",
    "\n",
    "# Create a train-test split (stratify on 'Publishable' as it is the binary target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y['Publishable'])\n",
    "\n",
    "# Combine the target variables into the training set for AutoGluon\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Train a model with AutoGluon for 'Publishable'\n",
    "publishable_predictor = TabularPredictor(label='Publishable').fit(train_data)\n",
    "\n",
    "# Train a model with AutoGluon for 'Conference'\n",
    "conference_predictor = TabularPredictor(label='Conference').fit(train_data)\n",
    "\n",
    "# Function to prepare data for prediction\n",
    "def prepare_data_for_prediction(df, predictor):\n",
    "    \"\"\"Ensure the input DataFrame matches the expected columns.\"\"\"\n",
    "    expected_columns = predictor.feature_metadata_in.get_features()\n",
    "    # Add missing columns with default values\n",
    "    for col in expected_columns:\n",
    "        if col not in df.columns:\n",
    "            df.loc[:, col] = None  # Use .loc to avoid SettingWithCopyWarning\n",
    "    return df[expected_columns]\n",
    "\n",
    "# Prepare data for predictions\n",
    "X_test_publishable = prepare_data_for_prediction(test_data[['Rationale']].copy(), publishable_predictor)\n",
    "X_test_conference = prepare_data_for_prediction(test_data[['Rationale']].copy(), conference_predictor)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_publishable = publishable_predictor.predict(X_test_publishable)\n",
    "y_pred_conference = conference_predictor.predict(X_test_conference)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Publishability Classification Report:\")\n",
    "print(classification_report(test_data['Publishable'], y_pred_publishable))\n",
    "\n",
    "print(\"Conference Classification Report:\")\n",
    "print(classification_report(test_data['Conference'], y_pred_conference))\n",
    "\n",
    "# Function for inference on a single document (PDF or DOCX)\n",
    "def classify_document(file_path):\n",
    "    # Check the file extension and process accordingly\n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if file_extension == '.pdf':\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    elif file_extension == '.docx':\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type: Only PDF and DOCX are supported.\")\n",
    "    \n",
    "    # Create a DataFrame with the text for prediction\n",
    "    sample_df = pd.DataFrame({'Rationale': [text]})\n",
    "    \n",
    "    # Prepare data for predictions\n",
    "    sample_df_publishable = prepare_data_for_prediction(sample_df.copy(), publishable_predictor)\n",
    "    sample_df_conference = prepare_data_for_prediction(sample_df.copy(), conference_predictor)\n",
    "    \n",
    "    # Predict publishability\n",
    "    publishable_prediction = publishable_predictor.predict(sample_df_publishable)\n",
    "    \n",
    "    if publishable_prediction.iloc[0] == 1:  # If it is publishable, predict the conference\n",
    "        conference_prediction = conference_predictor.predict(sample_df_conference)\n",
    "        return f\"Publishable, Conference: {conference_prediction.iloc[0]}\"\n",
    "    else:\n",
    "        return \"Non-Publishable\"\n",
    "\n",
    "# Example usage for a single document (PDF or DOCX)\n",
    "example_document_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\paper\\14.pdf\"\n",
    "result = classify_document(example_document_path)\n",
    "print(\"Classification Result for Example Document:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
